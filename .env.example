# ============================================
# Smart RAG Configuration
# ============================================
# Copy this file to .env and fill in your values
# Do NOT commit .env to version control

# ============================================
# Application Settings
# ============================================
APP_NAME=Smart RAG
APP_VERSION=0.1.0
APP_DEBUG=false
LOG_LEVEL=INFO

# ============================================
# OpenAI Configuration
# ============================================
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here

# Model Selection
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_EMBEDDING_MODEL=text-embedding-3-large

# Model Parameters
OPENAI_TEMPERATURE=0.0
OPENAI_MAX_TOKENS=2000

# ============================================
# Neo4j Database Configuration
# ============================================
# For Docker deployment (default)
NEO4J_URI=bolt://neo4j:7687
# For local deployment, use:
# NEO4J_URI=bolt://localhost:7687

NEO4J_USER=neo4j
# IMPORTANT: Change this to a secure password
NEO4J_PASSWORD=change-this-secure-password
NEO4J_DATABASE=neo4j

# ============================================
# Document Processing Settings
# ============================================
# Size of text chunks (in characters)
CHUNK_SIZE=1000

# Overlap between chunks (in characters)
CHUNK_OVERLAP=200

# Maximum number of entities to extract per chunk
MAX_ENTITIES_PER_CHUNK=50

# Number of chunks to process in a single embedding API call (higher = faster, more memory)
EMBEDDING_BATCH_SIZE=20

# ============================================
# Graph Construction Settings
# ============================================
# Minimum number of nodes in a community
MIN_COMMUNITY_SIZE=3

# Maximum levels in the hierarchical graph
MAX_HIERARCHY_LEVELS=3

# Similarity threshold for entity/chunk relationships (0.0-1.0)
SIMILARITY_THRESHOLD=0.7

# ============================================
# RAG (Retrieval-Augmented Generation) Settings
# ============================================
# Number of chunks to retrieve for a query
TOP_K_RETRIEVAL=10

# Number of top chunks after reranking
RERANK_TOP_K=5

# Maximum context length to send to LLM (in characters)
MAX_CONTEXT_LENGTH=4000
